


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch &mdash; MNIST sandbox, mlops CS course, 2023 0.0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://shiftlab.github.io/pytorch/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://shiftlab.github.io/pytorch/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/features">Features</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   
  <div>

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  0.0.1
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mnist_sandbox.html">mnist_sandbox package</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">

      <section data-toggle="wy-nav-shift" class="pytorch-content-wrap">
        <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
          <div class="pytorch-breadcrumbs-wrapper">
            















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="index.html">Module code</a> &gt;</li>
        
      <li>torch</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
          </div>

          <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
            Shortcuts
          </div>
        </div>

        <div class="pytorch-content-left">
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" class="pytorch-article">
              
  <h1>Source code for torch</h1><div class="highlight"><pre>
<span></span>
<span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">The torch package contains data structures for multi-dimensional</span>
<span class="sd">tensors and defines mathematical operations over these tensors.</span>
<span class="sd">Additionally, it provides many utilities for efficient serialization of</span>
<span class="sd">Tensors and arbitrary types, and other useful utilities.</span>

<span class="sd">It has a CUDA counterpart, that enables you to run your tensor computations</span>
<span class="sd">on an NVIDIA GPU with compute capability &gt;= 3.0.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">platform</span>
<span class="kn">import</span> <span class="nn">textwrap</span>
<span class="kn">import</span> <span class="nn">ctypes</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">version_info</span> <span class="o">&lt;</span> <span class="p">(</span><span class="mi">3</span><span class="p">,):</span>
    <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Python 2 has reached end-of-life and is no longer supported by PyTorch.&quot;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">._utils</span> <span class="kn">import</span> <span class="n">_import_dotted_name</span><span class="p">,</span> <span class="n">classproperty</span>
<span class="kn">from</span> <span class="nn">._utils_internal</span> <span class="kn">import</span> <span class="n">get_file_path</span><span class="p">,</span> <span class="n">prepare_multiprocessing_environment</span><span class="p">,</span> \
    <span class="n">USE_RTLD_GLOBAL_WITH_LIBTORCH</span><span class="p">,</span> <span class="n">USE_GLOBAL_DEPS</span>
<span class="c1"># TODO(torch_deploy) figure out how to freeze version.py in fbcode build</span>
<span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">executable</span> <span class="o">==</span> <span class="s1">&#39;torch_deploy&#39;</span><span class="p">:</span>
    <span class="n">__version__</span> <span class="o">=</span> <span class="s2">&quot;torch-deploy-1.8&quot;</span>
<span class="k">else</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">.torch_version</span> <span class="kn">import</span> <span class="n">__version__</span> <span class="k">as</span> <span class="n">__version__</span>

<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Set</span><span class="p">,</span> <span class="n">Type</span><span class="p">,</span> <span class="n">TYPE_CHECKING</span><span class="p">,</span> <span class="n">Union</span>
<span class="kn">import</span> <span class="nn">builtins</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;typename&#39;</span><span class="p">,</span> <span class="s1">&#39;is_tensor&#39;</span><span class="p">,</span> <span class="s1">&#39;is_storage&#39;</span><span class="p">,</span> <span class="s1">&#39;set_default_tensor_type&#39;</span><span class="p">,</span>
    <span class="s1">&#39;set_default_device&#39;</span><span class="p">,</span>
    <span class="s1">&#39;set_rng_state&#39;</span><span class="p">,</span> <span class="s1">&#39;get_rng_state&#39;</span><span class="p">,</span> <span class="s1">&#39;manual_seed&#39;</span><span class="p">,</span> <span class="s1">&#39;initial_seed&#39;</span><span class="p">,</span> <span class="s1">&#39;seed&#39;</span><span class="p">,</span>
    <span class="s1">&#39;save&#39;</span><span class="p">,</span> <span class="s1">&#39;load&#39;</span><span class="p">,</span> <span class="s1">&#39;set_printoptions&#39;</span><span class="p">,</span> <span class="s1">&#39;chunk&#39;</span><span class="p">,</span> <span class="s1">&#39;split&#39;</span><span class="p">,</span> <span class="s1">&#39;stack&#39;</span><span class="p">,</span> <span class="s1">&#39;matmul&#39;</span><span class="p">,</span>
    <span class="s1">&#39;no_grad&#39;</span><span class="p">,</span> <span class="s1">&#39;enable_grad&#39;</span><span class="p">,</span> <span class="s1">&#39;rand&#39;</span><span class="p">,</span> <span class="s1">&#39;randn&#39;</span><span class="p">,</span> <span class="s1">&#39;inference_mode&#39;</span><span class="p">,</span>
    <span class="s1">&#39;DoubleStorage&#39;</span><span class="p">,</span> <span class="s1">&#39;FloatStorage&#39;</span><span class="p">,</span> <span class="s1">&#39;LongStorage&#39;</span><span class="p">,</span> <span class="s1">&#39;IntStorage&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ShortStorage&#39;</span><span class="p">,</span> <span class="s1">&#39;CharStorage&#39;</span><span class="p">,</span> <span class="s1">&#39;ByteStorage&#39;</span><span class="p">,</span> <span class="s1">&#39;BoolStorage&#39;</span><span class="p">,</span>
    <span class="s1">&#39;TypedStorage&#39;</span><span class="p">,</span> <span class="s1">&#39;UntypedStorage&#39;</span><span class="p">,</span>
    <span class="s1">&#39;DoubleTensor&#39;</span><span class="p">,</span> <span class="s1">&#39;FloatTensor&#39;</span><span class="p">,</span> <span class="s1">&#39;LongTensor&#39;</span><span class="p">,</span> <span class="s1">&#39;IntTensor&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ShortTensor&#39;</span><span class="p">,</span> <span class="s1">&#39;CharTensor&#39;</span><span class="p">,</span> <span class="s1">&#39;ByteTensor&#39;</span><span class="p">,</span> <span class="s1">&#39;BoolTensor&#39;</span><span class="p">,</span> <span class="s1">&#39;Tensor&#39;</span><span class="p">,</span>
    <span class="s1">&#39;lobpcg&#39;</span><span class="p">,</span> <span class="s1">&#39;use_deterministic_algorithms&#39;</span><span class="p">,</span>
    <span class="s1">&#39;are_deterministic_algorithms_enabled&#39;</span><span class="p">,</span>
    <span class="s1">&#39;is_deterministic_algorithms_warn_only_enabled&#39;</span><span class="p">,</span>
    <span class="s1">&#39;set_deterministic_debug_mode&#39;</span><span class="p">,</span> <span class="s1">&#39;get_deterministic_debug_mode&#39;</span><span class="p">,</span>
    <span class="s1">&#39;set_float32_matmul_precision&#39;</span><span class="p">,</span> <span class="s1">&#39;get_float32_matmul_precision&#39;</span><span class="p">,</span>
    <span class="s1">&#39;set_warn_always&#39;</span><span class="p">,</span> <span class="s1">&#39;is_warn_always_enabled&#39;</span><span class="p">,</span> <span class="s1">&#39;SymInt&#39;</span><span class="p">,</span> <span class="s1">&#39;SymFloat&#39;</span><span class="p">,</span>
    <span class="s1">&#39;SymBool&#39;</span><span class="p">,</span> <span class="s1">&#39;sym_not&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sym_int&#39;</span><span class="p">,</span> <span class="s1">&#39;sym_float&#39;</span><span class="p">,</span> <span class="s1">&#39;sym_max&#39;</span><span class="p">,</span> <span class="s1">&#39;sym_min&#39;</span><span class="p">,</span> <span class="s1">&#39;compile&#39;</span><span class="p">,</span> <span class="s1">&#39;vmap&#39;</span>
<span class="p">]</span>

<span class="c1">################################################################################</span>
<span class="c1"># Load the extension module</span>
<span class="c1">################################################################################</span>

<span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">platform</span> <span class="o">==</span> <span class="s1">&#39;win32&#39;</span><span class="p">:</span>
    <span class="n">pfiles_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;ProgramFiles&#39;</span><span class="p">,</span> <span class="s1">&#39;C:</span><span class="se">\\</span><span class="s1">Program Files&#39;</span><span class="p">)</span>
    <span class="n">py_dll_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">exec_prefix</span><span class="p">,</span> <span class="s1">&#39;Library&#39;</span><span class="p">,</span> <span class="s1">&#39;bin&#39;</span><span class="p">)</span>
    <span class="n">th_dll_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="vm">__file__</span><span class="p">),</span> <span class="s1">&#39;lib&#39;</span><span class="p">)</span>

    <span class="c1"># When users create a virtualenv that inherits the base environment,</span>
    <span class="c1"># we will need to add the corresponding library directory into</span>
    <span class="c1"># DLL search directories. Otherwise, it will rely on `PATH` which</span>
    <span class="c1"># is dependent on user settings.</span>
    <span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">exec_prefix</span> <span class="o">!=</span> <span class="n">sys</span><span class="o">.</span><span class="n">base_exec_prefix</span><span class="p">:</span>
        <span class="n">base_py_dll_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">base_exec_prefix</span><span class="p">,</span> <span class="s1">&#39;Library&#39;</span><span class="p">,</span> <span class="s1">&#39;bin&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">base_py_dll_path</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

    <span class="n">dll_paths</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">,</span> <span class="p">[</span><span class="n">th_dll_path</span><span class="p">,</span> <span class="n">py_dll_path</span><span class="p">,</span> <span class="n">base_py_dll_path</span><span class="p">]))</span>

    <span class="k">if</span> <span class="nb">all</span><span class="p">([</span><span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s1">&#39;nvToolsExt64_1.dll&#39;</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">dll_paths</span><span class="p">]):</span>
        <span class="n">nvtoolsext_dll_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;NVTOOLSEXT_PATH&#39;</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pfiles_path</span><span class="p">,</span> <span class="s1">&#39;NVIDIA Corporation&#39;</span><span class="p">,</span> <span class="s1">&#39;NvToolsExt&#39;</span><span class="p">)),</span> <span class="s1">&#39;bin&#39;</span><span class="p">,</span> <span class="s1">&#39;x64&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">nvtoolsext_dll_path</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

    <span class="kn">from</span> <span class="nn">.version</span> <span class="kn">import</span> <span class="n">cuda</span> <span class="k">as</span> <span class="n">cuda_version</span>
    <span class="kn">import</span> <span class="nn">glob</span>
    <span class="k">if</span> <span class="n">cuda_version</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">([</span><span class="ow">not</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s1">&#39;cudart64*.dll&#39;</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">dll_paths</span><span class="p">]):</span>
        <span class="n">cuda_version_1</span> <span class="o">=</span> <span class="n">cuda_version</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="s1">&#39;_&#39;</span><span class="p">)</span>
        <span class="n">cuda_path_var</span> <span class="o">=</span> <span class="s1">&#39;CUDA_PATH_V&#39;</span> <span class="o">+</span> <span class="n">cuda_version_1</span>
        <span class="n">default_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pfiles_path</span><span class="p">,</span> <span class="s1">&#39;NVIDIA GPU Computing Toolkit&#39;</span><span class="p">,</span> <span class="s1">&#39;CUDA&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span> <span class="o">+</span> <span class="n">cuda_version</span><span class="p">)</span>
        <span class="n">cuda_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="n">cuda_path_var</span><span class="p">,</span> <span class="n">default_path</span><span class="p">),</span> <span class="s1">&#39;bin&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">cuda_path</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

    <span class="n">dll_paths</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">,</span> <span class="p">[</span><span class="n">nvtoolsext_dll_path</span><span class="p">,</span> <span class="n">cuda_path</span><span class="p">]))</span>

    <span class="n">kernel32</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">WinDLL</span><span class="p">(</span><span class="s1">&#39;kernel32.dll&#39;</span><span class="p">,</span> <span class="n">use_last_error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">with_load_library_flags</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">kernel32</span><span class="p">,</span> <span class="s1">&#39;AddDllDirectory&#39;</span><span class="p">)</span>
    <span class="n">prev_error_mode</span> <span class="o">=</span> <span class="n">kernel32</span><span class="o">.</span><span class="n">SetErrorMode</span><span class="p">(</span><span class="mh">0x0001</span><span class="p">)</span>

    <span class="n">kernel32</span><span class="o">.</span><span class="n">LoadLibraryW</span><span class="o">.</span><span class="n">restype</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span>
    <span class="k">if</span> <span class="n">with_load_library_flags</span><span class="p">:</span>
        <span class="n">kernel32</span><span class="o">.</span><span class="n">LoadLibraryExW</span><span class="o">.</span><span class="n">restype</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span>

    <span class="k">for</span> <span class="n">dll_path</span> <span class="ow">in</span> <span class="n">dll_paths</span><span class="p">:</span>
        <span class="n">os</span><span class="o">.</span><span class="n">add_dll_directory</span><span class="p">(</span><span class="n">dll_path</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">ctypes</span><span class="o">.</span><span class="n">CDLL</span><span class="p">(</span><span class="s1">&#39;vcruntime140.dll&#39;</span><span class="p">)</span>
        <span class="n">ctypes</span><span class="o">.</span><span class="n">CDLL</span><span class="p">(</span><span class="s1">&#39;msvcp140.dll&#39;</span><span class="p">)</span>
        <span class="n">ctypes</span><span class="o">.</span><span class="n">CDLL</span><span class="p">(</span><span class="s1">&#39;vcruntime140_1.dll&#39;</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">OSError</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;Microsoft Visual C++ Redistributable is not installed, this may lead to the DLL load failure.</span>
<span class="s1">                 It can be downloaded at https://aka.ms/vs/16/release/vc_redist.x64.exe&#39;&#39;&#39;</span><span class="p">)</span>

    <span class="n">dlls</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">th_dll_path</span><span class="p">,</span> <span class="s1">&#39;*.dll&#39;</span><span class="p">))</span>
    <span class="n">path_patched</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">dll</span> <span class="ow">in</span> <span class="n">dlls</span><span class="p">:</span>
        <span class="n">is_loaded</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">with_load_library_flags</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">kernel32</span><span class="o">.</span><span class="n">LoadLibraryExW</span><span class="p">(</span><span class="n">dll</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mh">0x00001100</span><span class="p">)</span>
            <span class="n">last_error</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">get_last_error</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">res</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">last_error</span> <span class="o">!=</span> <span class="mi">126</span><span class="p">:</span>
                <span class="n">err</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">WinError</span><span class="p">(</span><span class="n">last_error</span><span class="p">)</span>
                <span class="n">err</span><span class="o">.</span><span class="n">strerror</span> <span class="o">+=</span> <span class="sa">f</span><span class="s1">&#39; Error loading &quot;</span><span class="si">{</span><span class="n">dll</span><span class="si">}</span><span class="s1">&quot; or one of its dependencies.&#39;</span>
                <span class="k">raise</span> <span class="n">err</span>
            <span class="k">elif</span> <span class="n">res</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">is_loaded</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_loaded</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">path_patched</span><span class="p">:</span>
                <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;PATH&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dll_paths</span> <span class="o">+</span> <span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;PATH&#39;</span><span class="p">]])</span>
                <span class="n">path_patched</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">kernel32</span><span class="o">.</span><span class="n">LoadLibraryW</span><span class="p">(</span><span class="n">dll</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">res</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">err</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">WinError</span><span class="p">(</span><span class="n">ctypes</span><span class="o">.</span><span class="n">get_last_error</span><span class="p">())</span>
                <span class="n">err</span><span class="o">.</span><span class="n">strerror</span> <span class="o">+=</span> <span class="sa">f</span><span class="s1">&#39; Error loading &quot;</span><span class="si">{</span><span class="n">dll</span><span class="si">}</span><span class="s1">&quot; or one of its dependencies.&#39;</span>
                <span class="k">raise</span> <span class="n">err</span>

    <span class="n">kernel32</span><span class="o">.</span><span class="n">SetErrorMode</span><span class="p">(</span><span class="n">prev_error_mode</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_preload_cuda_deps</span><span class="p">(</span><span class="n">lib_folder</span><span class="p">,</span> <span class="n">lib_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Preloads cuda deps if they could not be found otherwise.&quot;&quot;&quot;</span>
    <span class="c1"># Should only be called on Linux if default path resolution have failed</span>
    <span class="k">assert</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;Linux&#39;</span><span class="p">,</span> <span class="s1">&#39;Should only be called on Linux&#39;</span>
    <span class="kn">import</span> <span class="nn">glob</span>
    <span class="n">lib_path</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">path</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="p">:</span>
        <span class="n">nvidia_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s1">&#39;nvidia&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">nvidia_path</span><span class="p">):</span>
            <span class="k">continue</span>
        <span class="n">candidate_lib_paths</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">nvidia_path</span><span class="p">,</span> <span class="n">lib_folder</span><span class="p">,</span> <span class="s1">&#39;lib&#39;</span><span class="p">,</span> <span class="n">lib_name</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">candidate_lib_paths</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">lib_path</span><span class="p">:</span>
            <span class="n">lib_path</span> <span class="o">=</span> <span class="n">candidate_lib_paths</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">lib_path</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">lib_path</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">lib_name</span><span class="si">}</span><span class="s2"> not found in the system path </span><span class="si">{</span><span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">ctypes</span><span class="o">.</span><span class="n">CDLL</span><span class="p">(</span><span class="n">lib_path</span><span class="p">)</span>


<span class="c1"># See Note [Global dependencies]</span>
<span class="k">def</span> <span class="nf">_load_global_deps</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">executable</span> <span class="o">==</span> <span class="s1">&#39;torch_deploy&#39;</span> <span class="ow">or</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;Windows&#39;</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="n">lib_name</span> <span class="o">=</span> <span class="s1">&#39;libtorch_global_deps&#39;</span> <span class="o">+</span> <span class="p">(</span><span class="s1">&#39;.dylib&#39;</span> <span class="k">if</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;Darwin&#39;</span> <span class="k">else</span> <span class="s1">&#39;.so&#39;</span><span class="p">)</span>
    <span class="n">here</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)</span>
    <span class="n">lib_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">here</span><span class="p">),</span> <span class="s1">&#39;lib&#39;</span><span class="p">,</span> <span class="n">lib_name</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">ctypes</span><span class="o">.</span><span class="n">CDLL</span><span class="p">(</span><span class="n">lib_path</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">ctypes</span><span class="o">.</span><span class="n">RTLD_GLOBAL</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">OSError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
        <span class="c1"># Can only happen for wheel with cuda libs as PYPI deps</span>
        <span class="c1"># As PyTorch is not purelib, but nvidia-*-cu11 is</span>
        <span class="n">cuda_libs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;cublas&#39;</span><span class="p">:</span> <span class="s1">&#39;libcublas.so.*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;cudnn&#39;</span><span class="p">:</span> <span class="s1">&#39;libcudnn.so.*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;cuda_nvrtc&#39;</span><span class="p">:</span> <span class="s1">&#39;libnvrtc.so.*[0-9].*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;cuda_runtime&#39;</span><span class="p">:</span> <span class="s1">&#39;libcudart.so.*[0-9].*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;cuda_cupti&#39;</span><span class="p">:</span> <span class="s1">&#39;libcupti.so.*[0-9].*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;cufft&#39;</span><span class="p">:</span> <span class="s1">&#39;libcufft.so.*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;curand&#39;</span><span class="p">:</span> <span class="s1">&#39;libcurand.so.*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;cusolver&#39;</span><span class="p">:</span> <span class="s1">&#39;libcusolver.so.*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;cusparse&#39;</span><span class="p">:</span> <span class="s1">&#39;libcusparse.so.*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;nccl&#39;</span><span class="p">:</span> <span class="s1">&#39;libnccl.so.*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;nvtx&#39;</span><span class="p">:</span> <span class="s1">&#39;libnvToolsExt.so.*[0-9]&#39;</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="n">is_cuda_lib_err</span> <span class="o">=</span> <span class="p">[</span><span class="n">lib</span> <span class="k">for</span> <span class="n">lib</span> <span class="ow">in</span> <span class="n">cuda_libs</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="k">if</span><span class="p">(</span><span class="n">lib</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">in</span> <span class="n">err</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_cuda_lib_err</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">err</span>
        <span class="k">for</span> <span class="n">lib_folder</span><span class="p">,</span> <span class="n">lib_name</span> <span class="ow">in</span> <span class="n">cuda_libs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">_preload_cuda_deps</span><span class="p">(</span><span class="n">lib_folder</span><span class="p">,</span> <span class="n">lib_name</span><span class="p">)</span>
        <span class="n">ctypes</span><span class="o">.</span><span class="n">CDLL</span><span class="p">(</span><span class="n">lib_path</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">ctypes</span><span class="o">.</span><span class="n">RTLD_GLOBAL</span><span class="p">)</span>


<span class="k">if</span> <span class="p">(</span><span class="n">USE_RTLD_GLOBAL_WITH_LIBTORCH</span> <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;TORCH_USE_RTLD_GLOBAL&#39;</span><span class="p">))</span> <span class="ow">and</span> \
        <span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">executable</span> <span class="o">==</span> <span class="s2">&quot;torch_deploy&quot;</span> <span class="ow">or</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span> <span class="o">!=</span> <span class="s1">&#39;Windows&#39;</span><span class="p">):</span>
    <span class="c1"># Do it the hard way.  You might want to load libtorch with RTLD_GLOBAL in a</span>
    <span class="c1"># few circumstances:</span>
    <span class="c1">#</span>
    <span class="c1">#   1. You&#39;re in a build environment (e.g., fbcode) where</span>
    <span class="c1">#      libtorch_global_deps is not available, but you still need</span>
    <span class="c1">#      to get mkl to link in with RTLD_GLOBAL or it will just</span>
    <span class="c1">#      not work.</span>
    <span class="c1">#</span>
    <span class="c1">#   2. You&#39;re trying to run PyTorch under UBSAN and you need</span>
    <span class="c1">#      to ensure that only one copy of libtorch is loaded, so</span>
    <span class="c1">#      vptr checks work properly</span>
    <span class="c1">#</span>
    <span class="c1"># If you&#39;re using this setting, you must verify that all the libraries</span>
    <span class="c1"># you load consistently use the same libstdc++, or you may have</span>
    <span class="c1"># mysterious segfaults.</span>
    <span class="c1">#</span>
    <span class="n">old_flags</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">getdlopenflags</span><span class="p">()</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">setdlopenflags</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">RTLD_GLOBAL</span> <span class="o">|</span> <span class="n">os</span><span class="o">.</span><span class="n">RTLD_LAZY</span><span class="p">)</span>
    <span class="kn">from</span> <span class="nn">torch._C</span> <span class="kn">import</span> <span class="o">*</span>  <span class="c1"># noqa: F403</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">setdlopenflags</span><span class="p">(</span><span class="n">old_flags</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">old_flags</span>

<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Easy way.  You want this most of the time, because it will prevent</span>
    <span class="c1"># C++ symbols from libtorch clobbering C++ symbols from other</span>
    <span class="c1"># libraries, leading to mysterious segfaults.</span>
    <span class="c1">#</span>
    <span class="c1"># If building in an environment where libtorch_global_deps isn&#39;t available</span>
    <span class="c1"># like parts of fbsource, but where RTLD_GLOBAL causes segfaults, you will</span>
    <span class="c1"># want USE_RTLD_GLOBAL_WITH_LIBTORCH = False and USE_GLOBAL_DEPS = False</span>
    <span class="c1">#</span>
    <span class="c1"># See Note [Global dependencies]</span>
    <span class="k">if</span> <span class="n">USE_GLOBAL_DEPS</span><span class="p">:</span>
        <span class="n">_load_global_deps</span><span class="p">()</span>
    <span class="kn">from</span> <span class="nn">torch._C</span> <span class="kn">import</span> <span class="o">*</span>  <span class="c1"># noqa: F403</span>

<span class="c1"># Appease the type checker; ordinarily this binding is inserted by the</span>
<span class="c1"># torch._C module initialization code in C</span>
<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">torch._C</span> <span class="k">as</span> <span class="nn">_C</span>

<span class="k">class</span> <span class="nc">SymInt</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Like an int (including magic methods), but redirects all operations on the</span>
<span class="sd">    wrapped node. This is used in particular to symbolically record operations</span>
<span class="sd">    in the symbolic shape workflow.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
        <span class="c1"># This field MUST be named node; C++ binding code assumes that this</span>
        <span class="c1"># class has a field named node that stores SymNode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">node</span> <span class="o">=</span> <span class="n">node</span>

    <span class="k">def</span> <span class="fm">__bool__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">bool_</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__int__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">int_</span><span class="p">()</span>

    <span class="c1"># Magic methods installed by torch.fx.experimental.symbolic_shapes</span>

    <span class="k">def</span> <span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="nb">object</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__lt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__gt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__le__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__ge__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__sym_max__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__sym_min__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__sym_float__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">SymFloat</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Like an float (including magic methods), but redirects all operations on the</span>
<span class="sd">    wrapped node. This is used in particular to symbolically record operations</span>
<span class="sd">    in the symbolic shape workflow.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">torch.fx.experimental.symbolic_shapes</span> <span class="kn">import</span> <span class="n">SymNode</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">SymNode</span><span class="p">)</span>
        <span class="c1"># This field MUST be named node; C++ binding code assumes that this</span>
        <span class="c1"># class has a field named node that stores SymNode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">node</span> <span class="o">=</span> <span class="n">node</span>

    <span class="k">def</span> <span class="fm">__bool__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">bool_</span><span class="p">()</span>

    <span class="c1"># Magic methods installed by torch.fx.experimental.symbolic_shapes</span>

    <span class="k">def</span> <span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="nb">object</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__lt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__gt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__le__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__ge__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__sym_max__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__sym_min__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__sym_int__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">str</span><span class="p">()</span>

<span class="k">class</span> <span class="nc">SymBool</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Like an bool (including magic methods), but redirects all operations on the</span>
<span class="sd">    wrapped node. This is used in particular to symbolically record operations</span>
<span class="sd">    in the symbolic shape workflow.</span>

<span class="sd">    Unlike regular bools, regular boolean operators will force extra guards instead</span>
<span class="sd">    of symbolically evaluate.  Use the bitwise operators instead to handle this.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">torch.fx.experimental.symbolic_shapes</span> <span class="kn">import</span> <span class="n">SymNode</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">SymNode</span><span class="p">)</span>
        <span class="c1"># This field MUST be named node; C++ binding code assumes that this</span>
        <span class="c1"># class has a field named node that stores SymNode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">node</span> <span class="o">=</span> <span class="n">node</span>

    <span class="k">def</span> <span class="fm">__bool__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">bool_</span><span class="p">()</span>

    <span class="c1"># Magic methods installed by torch.fx.experimental.symbolic_shapes</span>
    <span class="k">def</span> <span class="fm">__and__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SymBool&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__or__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SymBool&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="c1"># We very carefully define __sym_not__, and not a number of other</span>
    <span class="c1"># plausible alternatives:</span>
    <span class="c1">#</span>
    <span class="c1">#   - We do not override __not__ because this is not a real magic</span>
    <span class="c1">#     method; you cannot override the meaning of the not builtin in</span>
    <span class="c1">#     Python.  We use the name &#39;sym_not&#39; to clarify that in user code you</span>
    <span class="c1">#     cannot use the builtin not or operator.not_ or operator.__not__ and</span>
    <span class="c1">#     hit this magic method; you must use our custom sym_not operator.</span>
    <span class="c1">#</span>
    <span class="c1">#   - We do not override the __invert__ method because SymBool is</span>
    <span class="c1">#     meant to be usable in situations where bool is expected.  However,</span>
    <span class="c1">#     bitwise negation ~a does the wrong thing with booleans (because</span>
    <span class="c1">#     bool is a subclass of int, so ~1 = -2 which is not falseish.)</span>
    <span class="c1">#     This would be a giant footgun, so we get around it by defining</span>
    <span class="c1">#     our own operator.  Note that bitwise and/or do the right thing,</span>
    <span class="c1">#     so we reuse the conventional operators there for readability.</span>
    <span class="c1">#</span>
    <span class="k">def</span> <span class="nf">__sym_not__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SymBool&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">str</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">sym_not</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot; SymInt-aware utility for logical negation.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (SymBool or bool): Object to negate</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="s1">&#39;__sym_not__&#39;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">__sym_not__</span><span class="p">()</span>
    <span class="k">return</span> <span class="ow">not</span> <span class="n">a</span>

<span class="k">def</span> <span class="nf">sym_float</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot; SymInt-aware utility for float casting.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (SymInt, SymFloat, or object): Object to cast</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">SymFloat</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">a</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="s1">&#39;__sym_float__&#39;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">__sym_float__</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">py_float</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># type: ignore[operator]</span>


<span class="k">def</span> <span class="nf">sym_int</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot; SymInt-aware utility for int casting.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (SymInt, SymFloat, or object): Object to cast</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">SymInt</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">a</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">SymFloat</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="k">if</span> <span class="n">a</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>
    <span class="k">return</span> <span class="n">py_int</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># type: ignore[operator]</span>

<span class="k">def</span> <span class="nf">sym_max</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; SymInt-aware utility for max().&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="n">SymInt</span><span class="p">,</span> <span class="n">SymFloat</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">__sym_max__</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="p">(</span><span class="n">SymInt</span><span class="p">,</span> <span class="n">SymFloat</span><span class="p">)):</span>
        <span class="c1"># NB: If you actually care about preserving output type exactly</span>
        <span class="c1"># if you do something like max(0, 0.0), it is NOT sound to treat</span>
        <span class="c1"># min/max as commutative</span>
        <span class="k">return</span> <span class="n">b</span><span class="o">.</span><span class="n">__sym_max__</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">builtins</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>  <span class="c1"># type: ignore[operator]</span>

<span class="k">def</span> <span class="nf">sym_min</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; SymInt-aware utility for max().&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="n">SymInt</span><span class="p">,</span> <span class="n">SymFloat</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">__sym_min__</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="p">(</span><span class="n">SymInt</span><span class="p">,</span> <span class="n">SymFloat</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">b</span><span class="o">.</span><span class="n">__sym_min__</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">builtins</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>  <span class="c1"># type: ignore[operator]</span>

<span class="c1"># Check to see if we can load C extensions, and if not provide some guidance</span>
<span class="c1"># on what the problem might be.</span>
<span class="k">try</span><span class="p">:</span>
    <span class="c1"># _initExtension is chosen (arbitrarily) as a sentinel.</span>
    <span class="kn">from</span> <span class="nn">torch._C</span> <span class="kn">import</span> <span class="n">_initExtension</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">torch._C</span> <span class="k">as</span> <span class="nn">_C_for_compiled_check</span>

    <span class="c1"># The __file__ check only works for Python 3.7 and above.</span>
    <span class="k">if</span> <span class="n">_C_for_compiled_check</span><span class="o">.</span><span class="vm">__file__</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="n">textwrap</span><span class="o">.</span><span class="n">dedent</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">            Failed to load PyTorch C extensions:</span>
<span class="s1">                It appears that PyTorch has loaded the `torch/_C` folder</span>
<span class="s1">                of the PyTorch repository rather than the C extensions which</span>
<span class="s1">                are expected in the `torch._C` namespace. This can occur when</span>
<span class="s1">                using the `install` workflow. e.g.</span>
<span class="s1">                    $ python setup.py install &amp;&amp; python -c &quot;import torch&quot;</span>

<span class="s1">                This error can generally be solved using the `develop` workflow</span>
<span class="s1">                    $ python setup.py develop &amp;&amp; python -c &quot;import torch&quot;  # This should succeed</span>
<span class="s1">                or by running Python from a different directory.</span>
<span class="s1">            &#39;&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span> <span class="kn">from</span> <span class="kc">None</span>
    <span class="k">raise</span>  <span class="c1"># If __file__ is not None the cause is unknown, so just re-raise.</span>

<span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">_C</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">name</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="s1">&#39;_&#39;</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;Base&#39;</span><span class="p">):</span>
        <span class="n">__all__</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="n">obj</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">_C</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">Callable</span><span class="p">)</span> <span class="ow">or</span> <span class="n">inspect</span><span class="o">.</span><span class="n">isclass</span><span class="p">(</span><span class="n">obj</span><span class="p">)):</span>  <span class="c1"># type: ignore[arg-type]</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">obj</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">!=</span> <span class="s1">&#39;torch&#39;</span><span class="p">):</span>
                <span class="c1"># TODO: fix their module from C++ side</span>
                <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;DisableTorchFunctionSubclass&#39;</span><span class="p">,</span> <span class="s1">&#39;DisableTorchFunction&#39;</span><span class="p">,</span> <span class="s1">&#39;Generator&#39;</span><span class="p">]:</span>
                    <span class="n">obj</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">=</span> <span class="s1">&#39;torch&#39;</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="c1"># issue 38137 and python issue 43367. Submodules of a C extension are</span>
    <span class="c1"># non-standard, and attributes of those submodules cannot be pickled since</span>
    <span class="c1"># pickle expect to be able to import them as &quot;from _C.sub import attr&quot;</span>
    <span class="c1"># which fails with &quot;_C is not a package</span>
    <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">_C</span><span class="p">):</span>
        <span class="n">candidate</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">_C</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">candidate</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">type</span><span class="p">(</span><span class="n">_C</span><span class="p">):</span>
            <span class="c1"># submodule</span>
            <span class="k">if</span> <span class="sa">f</span><span class="s1">&#39;torch._C.</span><span class="si">{</span><span class="n">attr</span><span class="si">}</span><span class="s1">&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">:</span>
                <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;torch._C.</span><span class="si">{</span><span class="n">attr</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">candidate</span>


<span class="c1">################################################################################</span>
<span class="c1"># Define basic utilities</span>
<span class="c1">################################################################################</span>


<span class="k">def</span> <span class="nf">typename</span><span class="p">(</span><span class="n">o</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">o</span><span class="o">.</span><span class="n">type</span><span class="p">()</span>

    <span class="n">module</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
    <span class="n">class_name</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="s1">&#39;__module__&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">o</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">!=</span> <span class="s1">&#39;builtins&#39;</span> \
            <span class="ow">and</span> <span class="n">o</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">!=</span> <span class="s1">&#39;__builtin__&#39;</span> <span class="ow">and</span> <span class="n">o</span><span class="o">.</span><span class="vm">__module__</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">module</span> <span class="o">=</span> <span class="n">o</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="s1">&#39;__qualname__&#39;</span><span class="p">):</span>
        <span class="n">class_name</span> <span class="o">=</span> <span class="n">o</span><span class="o">.</span><span class="vm">__qualname__</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="s1">&#39;__name__&#39;</span><span class="p">):</span>
        <span class="n">class_name</span> <span class="o">=</span> <span class="n">o</span><span class="o">.</span><span class="vm">__name__</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">class_name</span> <span class="o">=</span> <span class="n">o</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="k">return</span> <span class="n">module</span> <span class="o">+</span> <span class="n">class_name</span>


<span class="k">def</span> <span class="nf">is_tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns True if `obj` is a PyTorch tensor.</span>

<span class="sd">    Note that this function is simply doing ``isinstance(obj, Tensor)``.</span>
<span class="sd">    Using that ``isinstance`` check is better for typechecking with mypy,</span>
<span class="sd">    and more explicit - so it&#39;s recommended to use that instead of</span>
<span class="sd">    ``is_tensor``.</span>

<span class="sd">    Args:</span>
<span class="sd">        obj (Object): Object to test</span>
<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; x = torch.tensor([1, 2, 3])</span>
<span class="sd">        &gt;&gt;&gt; torch.is_tensor(x)</span>
<span class="sd">        True</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">is_storage</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns True if `obj` is a PyTorch storage object.</span>

<span class="sd">    Args:</span>
<span class="sd">        obj (Object): Object to test</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">type</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="ow">in</span> <span class="n">_storage_classes</span>


<span class="n">_GLOBAL_DEVICE_CONTEXT</span> <span class="o">=</span> <span class="kc">None</span>

<span class="k">def</span> <span class="nf">set_default_device</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sets the default ``torch.Tensor`` to be allocated on ``device``.  This</span>
<span class="sd">    does not affect factory function calls which are called with an explicit</span>
<span class="sd">    ``device`` argument.  Factory calls will be performed as if they</span>
<span class="sd">    were passed ``device`` as an argument.</span>

<span class="sd">    To only temporarily change the default device instead of setting it</span>
<span class="sd">    globally, use ``with torch.device(device):`` instead.</span>

<span class="sd">    The default device is initially ``cpu``.  If you set the default tensor</span>
<span class="sd">    device to another device (e.g., ``cuda``) without a device index, tensors</span>
<span class="sd">    will be allocated on whatever the current device for the device type,</span>
<span class="sd">    even after :func:`torch.cuda.set_device` is called.</span>

<span class="sd">    .. warning::</span>

<span class="sd">        This function imposes a slight performance cost on every Python</span>
<span class="sd">        call to the torch API (not just factory functions).  If this</span>
<span class="sd">        is causing problems for you, please comment on</span>
<span class="sd">        https://github.com/pytorch/pytorch/issues/92701</span>

<span class="sd">    Args:</span>
<span class="sd">        device (device or string): the device to set as default</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;requires cuda, changes global state&quot;)</span>
<span class="sd">        &gt;&gt;&gt; torch.tensor([1.2, 3]).device</span>
<span class="sd">        device(type=&#39;cpu&#39;)</span>
<span class="sd">        &gt;&gt;&gt; torch.set_default_device(&#39;cuda&#39;)  # current device is 0</span>
<span class="sd">        &gt;&gt;&gt; torch.tensor([1.2, 3]).device</span>
<span class="sd">        device(type=&#39;cuda&#39;, index=0)</span>
<span class="sd">        &gt;&gt;&gt; torch.set_default_device(&#39;cuda:1&#39;)</span>
<span class="sd">        &gt;&gt;&gt; torch.tensor([1.2, 3]).device</span>
<span class="sd">        device(type=&#39;cuda&#39;, index=1)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_GLOBAL_DEVICE_CONTEXT</span>
    <span class="k">if</span> <span class="n">_GLOBAL_DEVICE_CONTEXT</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_GLOBAL_DEVICE_CONTEXT</span><span class="o">.</span><span class="fm">__exit__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_GLOBAL_DEVICE_CONTEXT</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">return</span>
    <span class="kn">from</span> <span class="nn">torch.utils._device</span> <span class="kn">import</span> <span class="n">DeviceContext</span>
    <span class="n">_GLOBAL_DEVICE_CONTEXT</span> <span class="o">=</span> <span class="n">DeviceContext</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">_GLOBAL_DEVICE_CONTEXT</span><span class="o">.</span><span class="fm">__enter__</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">set_default_tensor_type</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets the default ``torch.Tensor`` type to floating point tensor type</span>
<span class="sd">    ``t``. This type will also be used as default floating point type for</span>
<span class="sd">    type inference in :func:`torch.tensor`.</span>

<span class="sd">    The default floating point tensor type is initially ``torch.FloatTensor``.</span>

<span class="sd">    Args:</span>
<span class="sd">        t (type or string): the floating point tensor type or its name</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;Other tests may have changed the default type. Can we reset it?&quot;)</span>
<span class="sd">        &gt;&gt;&gt; torch.tensor([1.2, 3]).dtype    # initial default for floating point is torch.float32</span>
<span class="sd">        torch.float32</span>
<span class="sd">        &gt;&gt;&gt; torch.set_default_tensor_type(torch.DoubleTensor)</span>
<span class="sd">        &gt;&gt;&gt; torch.tensor([1.2, 3]).dtype    # a new floating point tensor</span>
<span class="sd">        torch.float64</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">_import_dotted_name</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="n">_C</span><span class="o">.</span><span class="n">_set_default_tensor_type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">set_default_dtype</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    Sets the default floating point dtype to :attr:`d`. Supports torch.float32</span>
<span class="sd">    and torch.float64 as inputs. Other dtypes may be accepted without complaint</span>
<span class="sd">    but are not supported and are unlikely to work as expected.</span>

<span class="sd">    When PyTorch is initialized its default floating point dtype is torch.float32,</span>
<span class="sd">    and the intent of set_default_dtype(torch.float64) is to facilitate NumPy-like</span>
<span class="sd">    type inference. The default floating point dtype is used to:</span>

<span class="sd">    1. Implicitly determine the default complex dtype. When the default floating point</span>
<span class="sd">       type is float32 the default complex dtype is complex64, and when the default</span>
<span class="sd">       floating point type is float64 the default complex type is complex128.</span>
<span class="sd">    2. Infer the dtype for tensors constructed using Python floats or complex Python</span>
<span class="sd">       numbers. See examples below.</span>
<span class="sd">    3. Determine the result of type promotion between bool and integer tensors and</span>
<span class="sd">       Python floats and complex Python numbers.</span>

<span class="sd">    Args:</span>
<span class="sd">        d (:class:`torch.dtype`): the floating point dtype to make the default.</span>
<span class="sd">                                  Either torch.float32 or torch.float64.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;Other tests may have changed the default type. Can we reset it?&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # initial default for floating point is torch.float32</span>
<span class="sd">        &gt;&gt;&gt; # Python floats are interpreted as float32</span>
<span class="sd">        &gt;&gt;&gt; torch.tensor([1.2, 3]).dtype</span>
<span class="sd">        torch.float32</span>
<span class="sd">        &gt;&gt;&gt; # initial default for floating point is torch.complex64</span>
<span class="sd">        &gt;&gt;&gt; # Complex Python numbers are interpreted as complex64</span>
<span class="sd">        &gt;&gt;&gt; torch.tensor([1.2, 3j]).dtype</span>
<span class="sd">        torch.complex64</span>

<span class="sd">        &gt;&gt;&gt; torch.set_default_dtype(torch.float64)</span>

<span class="sd">        &gt;&gt;&gt; # Python floats are now interpreted as float64</span>
<span class="sd">        &gt;&gt;&gt; torch.tensor([1.2, 3]).dtype    # a new floating point tensor</span>
<span class="sd">        torch.float64</span>
<span class="sd">        &gt;&gt;&gt; # Complex Python numbers are now interpreted as complex128</span>
<span class="sd">        &gt;&gt;&gt; torch.tensor([1.2, 3j]).dtype   # a new complex tensor</span>
<span class="sd">        torch.complex128</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_C</span><span class="o">.</span><span class="n">_set_default_dtype</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">use_deterministic_algorithms</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">warn_only</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot; Sets whether PyTorch operations must use &quot;deterministic&quot;</span>
<span class="sd">    algorithms. That is, algorithms which, given the same input, and when</span>
<span class="sd">    run on the same software and hardware, always produce the same output.</span>
<span class="sd">    When enabled, operations will use deterministic algorithms when available,</span>
<span class="sd">    and if only nondeterministic algorithms are available they will throw a</span>
<span class="sd">    :class:`RuntimeError` when called.</span>

<span class="sd">    .. note:: This setting alone is not always enough to make an application</span>
<span class="sd">        reproducible. Refer to :ref:`reproducibility` for more information.</span>

<span class="sd">    .. note:: :func:`torch.set_deterministic_debug_mode` offers an alternative</span>
<span class="sd">        interface for this feature.</span>

<span class="sd">    The following normally-nondeterministic operations will act</span>
<span class="sd">    deterministically when ``mode=True``:</span>

<span class="sd">        * :class:`torch.nn.Conv1d` when called on CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.Conv2d` when called on CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.Conv3d` when called on CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.ConvTranspose1d` when called on CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.ConvTranspose2d` when called on CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.ConvTranspose3d` when called on CUDA tensor</span>
<span class="sd">        * :func:`torch.bmm` when called on sparse-dense CUDA tensors</span>
<span class="sd">        * :func:`torch.Tensor.__getitem__` when attempting to differentiate a CPU tensor</span>
<span class="sd">          and the index is a list of tensors</span>
<span class="sd">        * :func:`torch.Tensor.index_put` with ``accumulate=False``</span>
<span class="sd">        * :func:`torch.Tensor.index_put` with ``accumulate=True`` when called on a CPU</span>
<span class="sd">          tensor</span>
<span class="sd">        * :func:`torch.Tensor.put_` with ``accumulate=True`` when called on a CPU</span>
<span class="sd">          tensor</span>
<span class="sd">        * :func:`torch.Tensor.scatter_add_` when called on a CUDA tensor</span>
<span class="sd">        * :func:`torch.gather` when called on a CUDA tensor that requires grad</span>
<span class="sd">        * :func:`torch.index_add` when called on CUDA tensor</span>
<span class="sd">        * :func:`torch.index_select` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :func:`torch.repeat_interleave` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :func:`torch.Tensor.index_copy` when called on a CPU or CUDA tensor</span>

<span class="sd">    The following normally-nondeterministic operations will throw a</span>
<span class="sd">    :class:`RuntimeError` when ``mode=True``:</span>

<span class="sd">        * :class:`torch.nn.AvgPool3d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.AdaptiveAvgPool2d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.AdaptiveAvgPool3d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.MaxPool3d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.AdaptiveMaxPool2d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.FractionalMaxPool2d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.FractionalMaxPool3d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.MaxUnpool1d`</span>
<span class="sd">        * :class:`torch.nn.MaxUnpool2d`</span>
<span class="sd">        * :class:`torch.nn.MaxUnpool3d`</span>
<span class="sd">        * :func:`torch.nn.functional.interpolate` when attempting to differentiate a CUDA tensor</span>
<span class="sd">          and one of the following modes is used:</span>

<span class="sd">          - ``linear``</span>
<span class="sd">          - ``bilinear``</span>
<span class="sd">          - ``bicubic``</span>
<span class="sd">          - ``trilinear``</span>

<span class="sd">        * :class:`torch.nn.ReflectionPad1d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.ReflectionPad2d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.ReflectionPad3d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.ReplicationPad1d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.ReplicationPad2d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.ReplicationPad3d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.NLLLoss` when called on a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.CTCLoss` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.EmbeddingBag` when attempting to differentiate a CUDA tensor when</span>
<span class="sd">          ``mode=&#39;max&#39;``</span>
<span class="sd">        * :func:`torch.Tensor.put_` when ``accumulate=False``</span>
<span class="sd">        * :func:`torch.Tensor.put_` when ``accumulate=True`` and called on a CUDA tensor</span>
<span class="sd">        * :func:`torch.histc` when called on a CUDA tensor</span>
<span class="sd">        * :func:`torch.bincount` when called on a CUDA tensor</span>
<span class="sd">        * :func:`torch.kthvalue` with called on a CUDA tensor</span>
<span class="sd">        * :func:`torch.median` with indices output when called on a CUDA tensor</span>
<span class="sd">        * :func:`torch.nn.functional.grid_sample` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :func:`torch.cumsum` when called on a CUDA tensor when dtype is floating point or complex</span>

<span class="sd">    A handful of CUDA operations are nondeterministic if the CUDA version is</span>
<span class="sd">    10.2 or greater, unless the environment variable ``CUBLAS_WORKSPACE_CONFIG=:4096:8``</span>
<span class="sd">    or ``CUBLAS_WORKSPACE_CONFIG=:16:8`` is set. See the CUDA documentation for more</span>
<span class="sd">    details: `&lt;https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility&gt;`_</span>
<span class="sd">    If one of these environment variable configurations is not set, a :class:`RuntimeError`</span>
<span class="sd">    will be raised from these operations when called with CUDA tensors:</span>

<span class="sd">        * :func:`torch.mm`</span>
<span class="sd">        * :func:`torch.mv`</span>
<span class="sd">        * :func:`torch.bmm`</span>

<span class="sd">    Note that deterministic operations tend to have worse performance than</span>
<span class="sd">    nondeterministic operations.</span>

<span class="sd">    .. note::</span>

<span class="sd">        This flag does not detect or prevent nondeterministic behavior caused</span>
<span class="sd">        by calling an inplace operation on a tensor with an internal memory</span>
<span class="sd">        overlap or by giving such a tensor as the :attr:`out` argument for an</span>
<span class="sd">        operation. In these cases, multiple writes of different data may target</span>
<span class="sd">        a single memory location, and the order of writes is not guaranteed.</span>

<span class="sd">    Args:</span>
<span class="sd">        mode (:class:`bool`): If True, makes potentially nondeterministic</span>
<span class="sd">            operations switch to a deterministic algorithm or throw a runtime</span>
<span class="sd">            error. If False, allows nondeterministic operations.</span>

<span class="sd">    Keyword args:</span>
<span class="sd">        warn_only (:class:`bool`, optional): If True, operations that do not</span>
<span class="sd">            have a deterministic implementation will throw a warning instead of</span>
<span class="sd">            an error. Default: ``False``</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; torch.use_deterministic_algorithms(True)</span>

<span class="sd">        # Forward mode nondeterministic error</span>
<span class="sd">        &gt;&gt;&gt; torch.randn(10, device=&#39;cuda&#39;).kthvalue(0)</span>
<span class="sd">        ...</span>
<span class="sd">        RuntimeError: kthvalue CUDA does not have a deterministic implementation...</span>

<span class="sd">        # Backward mode nondeterministic error</span>
<span class="sd">        &gt;&gt;&gt; torch.nn.AvgPool3d(1)(torch.randn(3, 4, 5, 6, requires_grad=True).cuda()).sum().backward()</span>
<span class="sd">        ...</span>
<span class="sd">        RuntimeError: avg_pool3d_backward_cuda does not have a deterministic implementation...</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_C</span><span class="o">.</span><span class="n">_set_deterministic_algorithms</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="n">warn_only</span><span class="o">=</span><span class="n">warn_only</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">are_deterministic_algorithms_enabled</span><span class="p">():</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns True if the global deterministic flag is turned on. Refer to</span>
<span class="sd">    :func:`torch.use_deterministic_algorithms` documentation for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_C</span><span class="o">.</span><span class="n">_get_deterministic_algorithms</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">is_deterministic_algorithms_warn_only_enabled</span><span class="p">():</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns True if the global deterministic flag is set to warn only.</span>
<span class="sd">    Refer to :func:`torch.use_deterministic_algorithms` documentation for more</span>
<span class="sd">    details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_C</span><span class="o">.</span><span class="n">_get_deterministic_algorithms_warn_only</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">set_deterministic_debug_mode</span><span class="p">(</span><span class="n">debug_mode</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">builtins</span><span class="o">.</span><span class="n">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets the debug mode for deterministic operations.</span>

<span class="sd">    .. note:: This is an alternative interface for</span>
<span class="sd">        :func:`torch.use_deterministic_algorithms`. Refer to that function&#39;s</span>
<span class="sd">        documentation for details about affected operations.</span>

<span class="sd">    Args:</span>
<span class="sd">        debug_mode(str or int): If &quot;default&quot; or 0, don&#39;t error or warn on</span>
<span class="sd">            nondeterministic operations. If &quot;warn&quot; or 1, warn on</span>
<span class="sd">            nondeterministic operations. If &quot;error&quot; or 2, error on</span>
<span class="sd">            nondeterministic operations.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># NOTE: builtins.int is used here because int in this scope resolves</span>
    <span class="c1"># to torch.int</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">debug_mode</span><span class="p">,</span> <span class="p">(</span><span class="n">builtins</span><span class="o">.</span><span class="n">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;debug_mode must be str or int, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">debug_mode</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">debug_mode</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">debug_mode</span> <span class="o">==</span> <span class="s1">&#39;default&#39;</span><span class="p">:</span>
            <span class="n">debug_mode</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="n">debug_mode</span> <span class="o">==</span> <span class="s1">&#39;warn&#39;</span><span class="p">:</span>
            <span class="n">debug_mode</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="n">debug_mode</span> <span class="o">==</span> <span class="s1">&#39;error&#39;</span><span class="p">:</span>
            <span class="n">debug_mode</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s1">&#39;invalid value of debug_mode, expected one of `default`, &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;`warn`, `error`, but got </span><span class="si">{</span><span class="n">debug_mode</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">debug_mode</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">_C</span><span class="o">.</span><span class="n">_set_deterministic_algorithms</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">debug_mode</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">_C</span><span class="o">.</span><span class="n">_set_deterministic_algorithms</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">warn_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">debug_mode</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">_C</span><span class="o">.</span><span class="n">_set_deterministic_algorithms</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s1">&#39;invalid value of debug_mode, expected 0, 1, or 2, &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;but got </span><span class="si">{</span><span class="n">debug_mode</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_deterministic_debug_mode</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">int</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the current value of the debug mode for deterministic</span>
<span class="sd">    operations. Refer to :func:`torch.set_deterministic_debug_mode`</span>
<span class="sd">    documentation for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">_C</span><span class="o">.</span><span class="n">_get_deterministic_algorithms</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">_C</span><span class="o">.</span><span class="n">_get_deterministic_algorithms_warn_only</span><span class="p">():</span>
            <span class="k">return</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>

<span class="k">def</span> <span class="nf">get_float32_matmul_precision</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">str</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the current value of float32 matrix multiplication precision. Refer to</span>
<span class="sd">    :func:`torch.set_float32_matmul_precision` documentation for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_C</span><span class="o">.</span><span class="n">_get_float32_matmul_precision</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">set_float32_matmul_precision</span><span class="p">(</span><span class="n">precision</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets the internal precision of float32 matrix multiplications.</span>

<span class="sd">    Running float32 matrix multiplications in lower precision may significantly increase</span>
<span class="sd">    performance, and in some programs the loss of precision has a negligible impact.</span>

<span class="sd">    Supports three settings:</span>

<span class="sd">        * &quot;highest&quot;, float32 matrix multiplications use the float32 datatype for</span>
<span class="sd">          internal computations.</span>
<span class="sd">        * &quot;high&quot;, float32 matrix multiplications use the TensorFloat32 or bfloat16_3x</span>
<span class="sd">          datatypes for internal computations, if fast matrix multiplication algorithms</span>
<span class="sd">          using those datatypes internally are available. Otherwise float32</span>
<span class="sd">          matrix multiplications are computed as if the precision is &quot;highest&quot;.</span>
<span class="sd">        * &quot;medium&quot;, float32 matrix multiplications use the bfloat16 datatype for</span>
<span class="sd">          internal computations, if a fast matrix multiplication algorithm</span>
<span class="sd">          using that datatype internally is available. Otherwise float32</span>
<span class="sd">          matrix multiplications are computed as if the precision is &quot;high&quot;.</span>

<span class="sd">    .. note::</span>

<span class="sd">        This does not change the output dtype of float32 matrix multiplications,</span>
<span class="sd">        it controls how the internal computation of the matrix multiplication is performed.</span>

<span class="sd">    .. note::</span>

<span class="sd">        This does not change the precision of convolution operations. Other flags,</span>
<span class="sd">        like `torch.backends.cudnn.allow_tf32`, may control the precision of convolution</span>
<span class="sd">        operations.</span>

<span class="sd">    .. note::</span>

<span class="sd">        This flag currently only affects one native device type: CUDA.</span>
<span class="sd">        If &quot;high&quot; or &quot;medium&quot; are set then the TensorFloat32 datatype will be used</span>
<span class="sd">        when computing float32 matrix multiplications, equivalent to setting</span>
<span class="sd">        `torch.backends.cuda.matmul.allow_tf32 = True`. When &quot;highest&quot; (the default)</span>
<span class="sd">        is set then the float32 datatype is used for internal computations, equivalent</span>
<span class="sd">        to setting `torch.backends.cuda.matmul.allow_tf32 = False`.</span>

<span class="sd">    Args:</span>
<span class="sd">        precision(str): can be set to &quot;highest&quot; (default), &quot;high&quot;, or &quot;medium&quot; (see above).</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_C</span><span class="o">.</span><span class="n">_set_float32_matmul_precision</span><span class="p">(</span><span class="n">precision</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">set_warn_always</span><span class="p">(</span><span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;When this flag is False (default) then some PyTorch warnings may only</span>
<span class="sd">    appear once per process. This helps avoid excessive warning information.</span>
<span class="sd">    Setting it to True causes these warnings to always appear, which may be</span>
<span class="sd">    helpful when debugging.</span>

<span class="sd">    Args:</span>
<span class="sd">        b (:class:`bool`): If True, force warnings to always be emitted</span>
<span class="sd">                           If False, set to the default behaviour</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_C</span><span class="o">.</span><span class="n">_set_warnAlways</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">is_warn_always_enabled</span><span class="p">():</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns True if the global warn_always flag is turned on. Refer to</span>
<span class="sd">    :func:`torch.set_warn_always` documentation for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_C</span><span class="o">.</span><span class="n">_get_warnAlways</span><span class="p">()</span>

<span class="c1">################################################################################</span>
<span class="c1"># Define numeric constants</span>
<span class="c1">################################################################################</span>

<span class="c1"># For Python Array API (https://data-apis.org/array-api/latest/API_specification/constants.html) and</span>
<span class="c1"># NumPy consistency (https://numpy.org/devdocs/reference/constants.html)</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">e</span> <span class="p">,</span> <span class="n">nan</span> <span class="p">,</span> <span class="n">inf</span> <span class="p">,</span> <span class="n">pi</span>
<span class="n">__all__</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="s1">&#39;e&#39;</span><span class="p">,</span> <span class="s1">&#39;pi&#39;</span><span class="p">,</span> <span class="s1">&#39;nan&#39;</span><span class="p">,</span> <span class="s1">&#39;inf&#39;</span><span class="p">])</span>

<span class="c1">################################################################################</span>
<span class="c1"># Define Storage and Tensor classes</span>
<span class="c1">################################################################################</span>

<span class="kn">from</span> <span class="nn">._tensor</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">.storage</span> <span class="kn">import</span> <span class="n">_StorageBase</span><span class="p">,</span> <span class="n">TypedStorage</span><span class="p">,</span> <span class="n">_LegacyStorage</span><span class="p">,</span> <span class="n">UntypedStorage</span><span class="p">,</span> <span class="n">_warn_typed_storage_removal</span>

<span class="c1"># NOTE: New &lt;type&gt;Storage classes should never be added. When adding a new</span>
<span class="c1"># dtype, use torch.storage.TypedStorage directly.</span>

<span class="k">class</span> <span class="nc">ByteStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span>

<span class="k">class</span> <span class="nc">DoubleStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">double</span>

<span class="k">class</span> <span class="nc">FloatStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span>

<span class="k">class</span> <span class="nc">HalfStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">half</span>

<span class="k">class</span> <span class="nc">LongStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">long</span>

<span class="k">class</span> <span class="nc">IntStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">int</span>

<span class="k">class</span> <span class="nc">ShortStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">short</span>

<span class="k">class</span> <span class="nc">CharStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>

<span class="k">class</span> <span class="nc">BoolStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">bool</span>

<span class="k">class</span> <span class="nc">BFloat16Storage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>

<span class="k">class</span> <span class="nc">ComplexDoubleStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cdouble</span>

<span class="k">class</span> <span class="nc">ComplexFloatStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cfloat</span>

<span class="k">class</span> <span class="nc">QUInt8Storage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">quint8</span>

<span class="k">class</span> <span class="nc">QInt8Storage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">qint8</span>

<span class="k">class</span> <span class="nc">QInt32Storage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">qint32</span>

<span class="k">class</span> <span class="nc">QUInt4x2Storage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">quint4x2</span>

<span class="k">class</span> <span class="nc">QUInt2x4Storage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">quint2x4</span>

<span class="n">_storage_classes</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">UntypedStorage</span><span class="p">,</span> <span class="n">DoubleStorage</span><span class="p">,</span> <span class="n">FloatStorage</span><span class="p">,</span> <span class="n">LongStorage</span><span class="p">,</span> <span class="n">IntStorage</span><span class="p">,</span>
    <span class="n">ShortStorage</span><span class="p">,</span> <span class="n">CharStorage</span><span class="p">,</span> <span class="n">ByteStorage</span><span class="p">,</span> <span class="n">HalfStorage</span><span class="p">,</span> <span class="n">BoolStorage</span><span class="p">,</span>
    <span class="n">QUInt8Storage</span><span class="p">,</span> <span class="n">QInt8Storage</span><span class="p">,</span> <span class="n">QInt32Storage</span><span class="p">,</span> <span class="n">BFloat16Storage</span><span class="p">,</span>
    <span class="n">ComplexFloatStorage</span><span class="p">,</span> <span class="n">ComplexDoubleStorage</span><span class="p">,</span> <span class="n">QUInt4x2Storage</span><span class="p">,</span> <span class="n">QUInt2x4Storage</span><span class="p">,</span>
    <span class="n">TypedStorage</span>
<span class="p">}</span>

<span class="c1"># The _tensor_classes set is initialized by the call to _C._initialize_tensor_type_bindings()</span>
<span class="n">_tensor_classes</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="n">Type</span><span class="p">]</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

<span class="c1"># If you edit these imports, please update torch/__init__.py.in as well</span>
<span class="kn">from</span> <span class="nn">.random</span> <span class="kn">import</span> <span class="n">set_rng_state</span><span class="p">,</span> <span class="n">get_rng_state</span><span class="p">,</span> <span class="n">manual_seed</span><span class="p">,</span> <span class="n">initial_seed</span><span class="p">,</span> <span class="n">seed</span>
<span class="kn">from</span> <span class="nn">.serialization</span> <span class="kn">import</span> <span class="n">save</span><span class="p">,</span> <span class="n">load</span>
<span class="kn">from</span> <span class="nn">._tensor_str</span> <span class="kn">import</span> <span class="n">set_printoptions</span>

<span class="c1">################################################################################</span>
<span class="c1"># Initialize extension</span>
<span class="c1">################################################################################</span>

<span class="k">def</span> <span class="nf">manager_path</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">executable</span> <span class="o">==</span> <span class="s1">&#39;torch_deploy&#39;</span> <span class="ow">or</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;Windows&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">b</span><span class="s2">&quot;&quot;</span>
    <span class="n">path</span> <span class="o">=</span> <span class="n">get_file_path</span><span class="p">(</span><span class="s1">&#39;torch&#39;</span><span class="p">,</span> <span class="s1">&#39;bin&#39;</span><span class="p">,</span> <span class="s1">&#39;torch_shm_manager&#39;</span><span class="p">)</span>
    <span class="n">prepare_multiprocessing_environment</span><span class="p">(</span><span class="n">get_file_path</span><span class="p">(</span><span class="s1">&#39;torch&#39;</span><span class="p">))</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Unable to find torch_shm_manager at &quot;</span> <span class="o">+</span> <span class="n">path</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">path</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">torch.amp</span> <span class="kn">import</span> <span class="n">autocast</span>

<span class="c1"># Initializing the extension shadows the built-in python float / int classes;</span>
<span class="c1"># store them for later use by SymInt / SymFloat.</span>
<span class="n">py_float</span> <span class="o">=</span> <span class="nb">float</span>
<span class="n">py_int</span> <span class="o">=</span> <span class="nb">int</span>

<span class="c1"># Shared memory manager needs to know the exact location of manager executable</span>
<span class="n">_C</span><span class="o">.</span><span class="n">_initExtension</span><span class="p">(</span><span class="n">manager_path</span><span class="p">())</span>
<span class="k">del</span> <span class="n">manager_path</span>

<span class="c1"># Appease the type checker: it can&#39;t deal with direct setting of globals().</span>
<span class="c1"># Note that we will see &quot;too many&quot; functions when reexporting this way; there</span>
<span class="c1"># is not a good way to fix this problem.  Perhaps, try to redesign VariableFunctions</span>
<span class="c1"># so that this import is good enough</span>
<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="c1"># Some type signatures pulled in from _VariableFunctions here clash with</span>
    <span class="c1"># signatures already imported. For now these clashes are ignored; see</span>
    <span class="c1"># PR #43339 for details.</span>
    <span class="kn">from</span> <span class="nn">torch._C._VariableFunctions</span> <span class="kn">import</span> <span class="o">*</span>  <span class="c1"># type: ignore[misc] # noqa: F403</span>
    <span class="c1"># Fixup segment_reduce visibility</span>
    <span class="n">_segment_reduce</span> <span class="o">=</span> <span class="n">segment_reduce</span>
    <span class="k">del</span> <span class="n">segment_reduce</span>

<span class="c1"># Ops not to be exposed in `torch` namespace,</span>
<span class="c1"># mostly helper ops.</span>
<span class="n">PRIVATE_OPS</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s1">&#39;unique_dim&#39;</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">_C</span><span class="o">.</span><span class="n">_VariableFunctions</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;__&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">PRIVATE_OPS</span><span class="p">:</span>
        <span class="k">continue</span>
    <span class="n">obj</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">_C</span><span class="o">.</span><span class="n">_VariableFunctions</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
    <span class="n">obj</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">=</span> <span class="s1">&#39;torch&#39;</span>
    <span class="c1"># Hide some APIs that should not be public</span>
    <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;segment_reduce&quot;</span><span class="p">:</span>
        <span class="c1"># TODO: Once the undocumented FC window is passed, remove the line bellow</span>
        <span class="nb">globals</span><span class="p">()[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">obj</span>
        <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="n">name</span>
    <span class="nb">globals</span><span class="p">()[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">obj</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">):</span>
        <span class="n">__all__</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

<span class="c1">################################################################################</span>
<span class="c1"># Import interface functions defined in Python</span>
<span class="c1">################################################################################</span>

<span class="c1"># needs to be after the above ATen bindings so we can overwrite from Python side</span>
<span class="kn">from</span> <span class="nn">.functional</span> <span class="kn">import</span> <span class="o">*</span>  <span class="c1"># noqa: F403</span>


<span class="c1">################################################################################</span>
<span class="c1"># Remove unnecessary members</span>
<span class="c1">################################################################################</span>

<span class="k">del</span> <span class="n">_StorageBase</span>
<span class="k">del</span> <span class="n">_LegacyStorage</span>

<span class="c1">################################################################################</span>
<span class="c1"># Define _assert</span>
<span class="c1">################################################################################</span>

<span class="c1"># needs to be before the submodule imports to avoid circular dependencies</span>
<span class="k">def</span> <span class="nf">_assert</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">message</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;A wrapper around Python&#39;s assert which is symbolically traceable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>

    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">condition</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="n">condition</span><span class="p">,)):</span>
        <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">_assert</span><span class="p">,</span> <span class="p">(</span><span class="n">condition</span><span class="p">,),</span> <span class="n">condition</span><span class="p">,</span> <span class="n">message</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">condition</span><span class="p">,</span> <span class="n">message</span>

<span class="c1">################################################################################</span>
<span class="c1"># Import most common subpackages</span>
<span class="c1">################################################################################</span>

<span class="c1"># Use the redundant form so that type checkers know that these are a part of</span>
<span class="c1"># the public API. The &quot;regular&quot; import lines are there solely for the runtime</span>
<span class="c1"># side effect of adding to the imported module&#39;s members for other users.</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">cuda</span> <span class="k">as</span> <span class="n">cuda</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">cpu</span> <span class="k">as</span> <span class="n">cpu</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">autograd</span> <span class="k">as</span> <span class="n">autograd</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">no_grad</span> <span class="k">as</span> <span class="n">no_grad</span><span class="p">,</span>
    <span class="n">enable_grad</span> <span class="k">as</span> <span class="n">enable_grad</span><span class="p">,</span>
    <span class="n">set_grad_enabled</span> <span class="k">as</span> <span class="n">set_grad_enabled</span><span class="p">,</span>
    <span class="n">inference_mode</span> <span class="k">as</span> <span class="n">inference_mode</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">fft</span> <span class="k">as</span> <span class="n">fft</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">futures</span> <span class="k">as</span> <span class="n">futures</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">_awaits</span> <span class="k">as</span> <span class="n">_awaits</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nested</span> <span class="k">as</span> <span class="n">nested</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.signal</span> <span class="kn">import</span> <span class="n">windows</span> <span class="k">as</span> <span class="n">windows</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="nn">torch.optim._multi_tensor</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">multiprocessing</span> <span class="k">as</span> <span class="n">multiprocessing</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">sparse</span> <span class="k">as</span> <span class="n">sparse</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">special</span> <span class="k">as</span> <span class="n">special</span>
<span class="kn">import</span> <span class="nn">torch.utils.backcompat</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">onnx</span> <span class="k">as</span> <span class="n">onnx</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">jit</span> <span class="k">as</span> <span class="n">jit</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">linalg</span> <span class="k">as</span> <span class="n">linalg</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">hub</span> <span class="k">as</span> <span class="n">hub</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">random</span> <span class="k">as</span> <span class="n">random</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">distributions</span> <span class="k">as</span> <span class="n">distributions</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">testing</span> <span class="k">as</span> <span class="n">testing</span>
<span class="kn">import</span> <span class="nn">torch.backends.cuda</span>
<span class="kn">import</span> <span class="nn">torch.backends.mps</span>
<span class="kn">import</span> <span class="nn">torch.backends.cudnn</span>
<span class="kn">import</span> <span class="nn">torch.backends.mkl</span>
<span class="kn">import</span> <span class="nn">torch.backends.mkldnn</span>
<span class="kn">import</span> <span class="nn">torch.backends.openmp</span>
<span class="kn">import</span> <span class="nn">torch.backends.quantized</span>
<span class="kn">import</span> <span class="nn">torch.utils.data</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">__config__</span> <span class="k">as</span> <span class="n">__config__</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">__future__</span> <span class="k">as</span> <span class="n">__future__</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">profiler</span> <span class="k">as</span> <span class="n">profiler</span>

<span class="c1"># Quantized, sparse, AO, etc. should be last to get imported, as nothing</span>
<span class="c1"># is expected to depend on them.</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">ao</span> <span class="k">as</span> <span class="n">ao</span>
<span class="c1"># nn.quant* depends on ao -- so should be after those.</span>
<span class="kn">import</span> <span class="nn">torch.nn.quantizable</span>
<span class="kn">import</span> <span class="nn">torch.nn.quantized</span>
<span class="kn">import</span> <span class="nn">torch.nn.qat</span>
<span class="kn">import</span> <span class="nn">torch.nn.intrinsic</span>

<span class="n">_C</span><span class="o">.</span><span class="n">_init_names</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_storage_classes</span><span class="p">))</span>

<span class="c1"># attach docstrings to torch and tensor functions</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">_torch_docs</span><span class="p">,</span> <span class="n">_tensor_docs</span><span class="p">,</span> <span class="n">_storage_docs</span>
<span class="k">del</span> <span class="n">_torch_docs</span><span class="p">,</span> <span class="n">_tensor_docs</span><span class="p">,</span> <span class="n">_storage_docs</span>


<span class="k">def</span> <span class="nf">compiled_with_cxx11_abi</span><span class="p">():</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_C</span><span class="o">.</span><span class="n">_GLIBCXX_USE_CXX11_ABI</span>


<span class="c1"># Import the ops &quot;namespace&quot;</span>
<span class="kn">from</span> <span class="nn">torch._ops</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">torch._classes</span> <span class="kn">import</span> <span class="n">classes</span>

<span class="c1"># quantization depends on torch.fx</span>
<span class="c1"># Import quantization</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">quantization</span> <span class="k">as</span> <span class="n">quantization</span>

<span class="c1"># Import the quasi random sampler</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">quasirandom</span> <span class="k">as</span> <span class="n">quasirandom</span>

<span class="c1"># If you are seeing this, it means that this call site was not checked if</span>
<span class="c1"># the memory format could be preserved, and it was switched to old default</span>
<span class="c1"># behaviour of contiguous</span>
<span class="n">legacy_contiguous_format</span> <span class="o">=</span> <span class="n">contiguous_format</span>

<span class="c1"># Register fork handler to initialize OpenMP in child processes (see gh-28389)</span>
<span class="kn">from</span> <span class="nn">torch.multiprocessing._atfork</span> <span class="kn">import</span> <span class="n">register_after_fork</span>
<span class="n">register_after_fork</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">get_num_threads</span><span class="p">)</span>
<span class="k">del</span> <span class="n">register_after_fork</span>

<span class="c1"># Import tools that require fully imported torch (for applying</span>
<span class="c1"># torch.jit.script as a decorator, for instance):</span>
<span class="kn">from</span> <span class="nn">._lobpcg</span> <span class="kn">import</span> <span class="n">lobpcg</span> <span class="k">as</span> <span class="n">lobpcg</span>

<span class="c1"># These were previously defined in native_functions.yaml and appeared on the</span>
<span class="c1"># `torch` namespace, but we moved them to c10 dispatch to facilitate custom</span>
<span class="c1"># class usage. We add these lines here to preserve backward compatibility.</span>
<span class="n">quantized_lstm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">quantized_lstm</span>
<span class="n">quantized_gru</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">quantized_gru</span>

<span class="kn">from</span> <span class="nn">torch.utils.dlpack</span> <span class="kn">import</span> <span class="n">from_dlpack</span><span class="p">,</span> <span class="n">to_dlpack</span>

<span class="c1"># Import experimental masked operations support. See</span>
<span class="c1"># [RFC-0016](https://github.com/pytorch/rfcs/pull/27) for more</span>
<span class="c1"># information.</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">masked</span>

<span class="c1"># Import removed ops with error message about removal</span>
<span class="kn">from</span> <span class="nn">._linalg_utils</span> <span class="kn">import</span> <span class="p">(</span>  <span class="c1"># type: ignore[misc]</span>
    <span class="n">matrix_rank</span><span class="p">,</span>
    <span class="n">eig</span><span class="p">,</span>
    <span class="n">solve</span><span class="p">,</span>
    <span class="n">lstsq</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">._linalg_utils</span> <span class="kn">import</span> <span class="n">_symeig</span> <span class="k">as</span> <span class="n">symeig</span>  <span class="c1"># type: ignore[misc]</span>


<span class="k">class</span> <span class="nc">_TorchCompileInductorWrapper</span><span class="p">:</span>
    <span class="n">compiler_name</span> <span class="o">=</span> <span class="s2">&quot;inductor&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">options</span><span class="p">,</span> <span class="n">dynamic</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dynamic</span> <span class="o">=</span> <span class="n">dynamic</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">apply_mode</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">apply_options</span><span class="p">(</span><span class="n">options</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">dynamic</span><span class="p">:</span>
            <span class="c1"># cudagraphs conflicts with dynamic shapes</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;triton.cudagraphs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">assert</span> <span class="s2">&quot;triton.cudagraphs&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span>
                <span class="n">options</span> <span class="ow">or</span> <span class="p">()</span>
            <span class="p">),</span> <span class="s2">&quot;triton.cudagraphs does not support dynamic shapes&quot;</span>

    <span class="k">def</span> <span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">_TorchCompileInductorWrapper</span><span class="p">)</span> <span class="ow">and</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">config</span> <span class="ow">and</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dynamic</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">dynamic</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">apply_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]):</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;default&quot;</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;reduce-overhead&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">apply_options</span><span class="p">({</span>
                <span class="s2">&quot;triton.cudagraphs&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                <span class="s2">&quot;size_asserts&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="p">})</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;max-autotune&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">apply_options</span><span class="p">({</span>
                <span class="s2">&quot;epilogue_fusion&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                <span class="s2">&quot;max_autotune&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                <span class="s2">&quot;triton.cudagraphs&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
            <span class="p">})</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Unrecognized mode=</span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">, should be one of: default, reduce-overhead, max-autotune&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">apply_options</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">options</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">options</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="kn">from</span> <span class="nn">torch._inductor</span> <span class="kn">import</span> <span class="n">config</span>
        <span class="n">current_config</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>  <span class="c1"># type: ignore[attr-defined]</span>

        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">options</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">attr_name</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">attr_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">current_config</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Unexpected optimization option </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">, known options are </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">current_config</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">type</span><span class="p">(</span><span class="n">current_config</span><span class="p">[</span><span class="n">attr_name</span><span class="p">]):</span>
                <span class="n">val_type_str</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
                <span class="n">expected_type_str</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">current_config</span><span class="p">[</span><span class="n">attr_name</span><span class="p">])</span><span class="o">.</span><span class="vm">__name__</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Unexpected type of attr </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">val_type_str</span><span class="si">}</span><span class="s2"> should be </span><span class="si">{</span><span class="n">expected_type_str</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="n">attr_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_</span><span class="p">,</span> <span class="n">inputs_</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">torch._inductor.compile_fx</span> <span class="kn">import</span> <span class="n">compile_fx</span>

        <span class="k">return</span> <span class="n">compile_fx</span><span class="p">(</span><span class="n">model_</span><span class="p">,</span> <span class="n">inputs_</span><span class="p">,</span> <span class="n">config_patches</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">compile</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span>
            <span class="n">fullgraph</span><span class="p">:</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">dynamic</span><span class="p">:</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">backend</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;inductor&quot;</span><span class="p">,</span>
            <span class="n">mode</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">options</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">builtins</span><span class="o">.</span><span class="n">int</span><span class="p">,</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">disable</span><span class="p">:</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Optimizes given model/function using TorchDynamo and specified backend.</span>

<span class="sd">    Args:</span>
<span class="sd">       model (Callable): Module/function to optimize</span>
<span class="sd">       fullgraph (bool): Whether it is ok to break model into several subgraphs</span>
<span class="sd">       dynamic (bool): Use dynamic shape tracing</span>
<span class="sd">       backend (str or Callable): backend to be used</span>
<span class="sd">       mode (str): Can be either &quot;default&quot;, &quot;reduce-overhead&quot; or &quot;max-autotune&quot;</span>
<span class="sd">       options (dict): A dictionary of options to pass to the backend.</span>
<span class="sd">       disable (bool): Turn torch.compile() into a no-op for testing</span>

<span class="sd">    Example::</span>

<span class="sd">        @torch.compile(options={&quot;matmul-padding&quot;: True}, fullgraph=True)</span>
<span class="sd">        def foo(x):</span>
<span class="sd">            return torch.sin(x) + torch.cos(x)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;torch.compile&quot;</span><span class="p">)</span>
    <span class="c1"># Decorator mode</span>
    <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">Callable</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Model can&#39;t be None&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="nb">compile</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
                           <span class="n">fullgraph</span><span class="o">=</span><span class="n">fullgraph</span><span class="p">,</span>
                           <span class="n">dynamic</span><span class="o">=</span><span class="n">dynamic</span><span class="p">,</span>
                           <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>
                           <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span>
                           <span class="n">options</span><span class="o">=</span><span class="n">options</span><span class="p">,</span>
                           <span class="n">disable</span><span class="o">=</span><span class="n">disable</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">fn</span>

    <span class="kn">import</span> <span class="nn">torch._dynamo</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">options</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Either mode or options can be specified, but both can&#39;t be specified at the same time.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">options</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mode</span> <span class="o">=</span> <span class="s2">&quot;default&quot;</span>
    <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;inductor&quot;</span><span class="p">:</span>
        <span class="n">backend</span> <span class="o">=</span> <span class="n">_TorchCompileInductorWrapper</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="n">options</span><span class="p">,</span> <span class="n">dynamic</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span> <span class="n">nopython</span><span class="o">=</span><span class="n">fullgraph</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="n">dynamic</span><span class="p">,</span> <span class="n">disable</span><span class="o">=</span><span class="n">disable</span><span class="p">)(</span><span class="n">model</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_register_device_module</span><span class="p">(</span><span class="n">device_type</span><span class="p">,</span> <span class="n">module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Register an external runtime module of the specific :attr:`device_type`</span>
<span class="sd">    supported by torch.</span>

<span class="sd">    After the :attr:`module` is registered correctly, the user can refer</span>
<span class="sd">    the external runtime module as part of torch with attribute torch.xxx.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Make sure the device_type represent a supported device type for torch.</span>
    <span class="n">device_type</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device_type</span><span class="p">)</span><span class="o">.</span><span class="n">type</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">device_type</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;The runtime module of &#39;</span><span class="si">{}</span><span class="s2">&#39; has already &quot;</span>
                           <span class="s2">&quot;been registered with &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">device_type</span><span class="p">,</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">device_type</span><span class="p">)))</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">device_type</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="n">torch_module_name</span> <span class="o">=</span> <span class="s1">&#39;.&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">device_type</span><span class="p">])</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="n">torch_module_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span>

<span class="c1"># expose return_types</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">return_types</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">library</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">_meta_registrations</span>

<span class="c1"># Enable CUDA Sanitizer</span>
<span class="k">if</span> <span class="s1">&#39;TORCH_CUDA_SANITIZER&#39;</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">torch.cuda._sanitizer</span> <span class="k">as</span> <span class="nn">csan</span>

    <span class="n">csan</span><span class="o">.</span><span class="n">enable_cuda_sanitizer</span><span class="p">()</span>

<span class="c1"># Populate magic methods on SymInt and SymFloat</span>
<span class="kn">import</span> <span class="nn">torch.fx.experimental.symbolic_shapes</span>

<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">func</span> <span class="k">as</span> <span class="n">func</span>
<span class="kn">from</span> <span class="nn">torch.func</span> <span class="kn">import</span> <span class="n">vmap</span>

<span class="c1"># The function _sparse_coo_tensor_unsafe is removed from PyTorch</span>
<span class="c1"># Python API (v. 1.13), here we temporarily provide its replacement</span>
<span class="c1"># with a deprecation warning.</span>
<span class="c1"># TODO: remove the function for PyTorch v 1.15.</span>
<span class="k">def</span> <span class="nf">_sparse_coo_tensor_unsafe</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">warnings</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;torch._sparse_coo_tensor_unsafe is deprecated, &#39;</span>
                  <span class="s1">&#39;use torch.sparse_coo_tensor(..., check_invariants=False) instead.&#39;</span><span class="p">)</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;check_invariants&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, &lt;unk&gt;.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>
  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'0.0.1',
            LANGUAGE:'en',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="../_static/sphinx_highlight.js"></script>
      <script type="text/javascript" src="../_static/design-tabs.js"></script>

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Lorem ipsum dolor sit amet, consectetur</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Lorem ipsum dolor sit amet, consectetur</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Lorem ipsum dolor sit amet, consectetur</p>
          <a class="with-right-arrow" href="https://shiftlab.github.io/pytorch/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://shiftlab.github.io/pytorch/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://shiftlab.github.io/pytorch/">PyTorch</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/get-started">Get Started</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/features">Features</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/ecosystem">Ecosystem</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/blog/">Blog</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/resources">Resources</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://shiftlab.github.io/pytorch/support">Support</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.slack.com" target="_blank">Slack</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Follow Us</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://shiftlab.github.io/pytorch/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>

          <li>
            <a href="#">Features</a>
          </li>

          <li>
            <a href="#">Ecosystem</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    mobileMenu.bind();
    mobileTOC.bind();
    pytorchAnchors.bind();

    $(window).on("load", function() {
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
    })

    // Add class to links that have code blocks, since we cannot create links in code blocks
    $("article.pytorch-article a span.pre").each(function(e) {
      $(this).closest("a").addClass("has-code");
    });
  </script>
</body>
</html>